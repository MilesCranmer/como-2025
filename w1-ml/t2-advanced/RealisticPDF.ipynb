{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0c38164",
   "metadata": {},
   "source": [
    "_This notebook is part of the material for the [ML Tutorials](https://github.com/NNPDF/como-2025) session._\n",
    "\n",
    "# Real life PDF fit\n",
    "\n",
    "In the previous tutorial (PDF Fitting) we have fitted a Neural Network to PDF data that we have obtained from LHAPDF. Ah! If life were so simple!\n",
    "\n",
    "Reality is much more complicated:\n",
    "\n",
    "1. We cannot measure the PDF: we have no PDF data!\n",
    "\n",
    "2. The data has some uncertainties associated to it.\n",
    "\n",
    "In this tutorial we are going to do a more realistic (albeit simplified) PDF fit.\n",
    "\n",
    "## From the PDF to the experimental data\n",
    "\n",
    "In an experiment we only have access to observables. These observables, while they can be computed theoretically, depend on the PDF in a non trivial manner, for hadronic collision (such as those at the LHC) we have:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathcal{O} = \\displaystyle\\sum_{ij} \\int dx_{1} dx_{2}  \\  f_{i}  (x_1, \\mu_F) \\  f_{j}(x_2, \\mu_F) \\ \\hat{\\sigma}_{ij}(x_{1}, x_{2}, \\mu_{R}, \\mu_{F})\n",
    "\\end{equation}\n",
    "\n",
    "For simplicity (and because that topic was already covered in the tutorials of the third day) we are going to drop the dependence on $\\mu_F$ from the PDF. All scale-dependence is contained in the partonic cross section instead\n",
    "\n",
    "Utilizing the model of the PDF that we built in the previous tutorial our PDF have instead the following form: $f_{i}(x) = (1 - x)^{1+\\beta}NN_{i}(x)$ where the index $i$ refers to the parton.\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathcal{O} = \\displaystyle\\sum_{ij} \\int dx_{1} dx_{2}  \\ (1 - x_1)^{1+\\beta}NN_{i}(x_1) \\ (1 - x_2)^{1+\\beta}NN_{j}(x_2) \\ \\hat{\\sigma}_{ij}(x_{1}, x_{2}, \\mu_{R}, \\mu_{F})\n",
    "\\end{equation}\n",
    "\n",
    "Note that in the previous equation both Neural Networks are the same, but are evaluated at different values of $x$ (and potentially contribute with different partons $i$ and $j$). This means the observable depends non-linearly on the Neural Network, which greatly complicates the training. For this tutorial we are going to limit ourselves to DIS observables so one of the two PDFs is set to 1, which will facilitate the construction of the network, but in a global PDF fit both DIS and double-hadronic observables need to be considered.\n",
    "\n",
    "Up to this point we have used Mean Squared Errors as the loss function to be optimized. In this case we are comparing datapoints ($D$) with observables ($\\mathcal{O}$). The loss function thus looks like:\n",
    "\n",
    "\\begin{equation}\n",
    "    L = \\frac{1}{N}\\sum_{k} (\\mathcal{O}_{k} - D_{k})^2 = \\frac{1}{N}\\sum_{k}\\left(\\displaystyle\\sum_{ij} \\int dx_{1} dx_{2}  \\ (1 - x_1)^{1+\\beta}NN_{i}(x_1) \\ (1 - x_2)^{1+\\beta}NN_{j}(x_2) \\ \\hat{\\sigma}_{ij}^{(k)}(x_{1}, x_{2}, \\mu_{R}, \\mu_{F}) - D_{k}\\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "with $k$ running over datapoints in the fit. This loss function corresponds and it is usually called $\\chi^{2}$.\n",
    "\n",
    "\n",
    "## Experimental uncertainties\n",
    "\n",
    "In addition to the previous consideration, we need to include the information about the experimental uncertainties. Experimental uncertainties introduce correlations between datapoints which need to be taken into account:\n",
    "\n",
    "\\begin{equation}\n",
    "    L = \\frac{1}{N}\\sum_{k,l} (\\mathcal{O}_{k} - D_{k})s_{kl}^{-1}(\\mathcal{O}_{l} - D_{l})\n",
    "\\end{equation}\n",
    "\n",
    "With $s_{kl}^{-1}$ the inverse of the covariance matrix. In the limit of a diagional covariance matrix (no correlation between datapoints) one would recover the simpler form that we have used before.\n",
    "\n",
    "## Tutorial outline\n",
    "\n",
    "In this tutorial we are going to take the final multi-flavour PDF we constructed in the previous one as the starting point.\n",
    "\n",
    "In order to have a realistic-looking PDF from only a few datasets, we are going to use the `.npz` files that you downloaded in the first day of the school. They contain:\n",
    "\n",
    "- `D`: the experimental data\n",
    "- `covmat`: the experimetal covariance matrix\n",
    "- `fktable`: an interpolation table for the partonic cross section the fktable is a tensor `(ndata, luminosity channel, x)`\n",
    "- `xgrid`: grid in x in which to evaluate the PDF\n",
    "- `luminosity`: the relevant indices of the luminosity\n",
    "\n",
    "The first thing we will do is to create an observable that we can compare to data. In a first step we will simply compare to data without taking into account the experimental uncertainties. \n",
    "\n",
    "Then we will create a custom loss function, introducing the covariance matrix into the problem.\n",
    "\n",
    "We will finish the tutorials creating replicas of the data to generate a PDF ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea65cf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "data_folder = Path(\"data\") / \"pdf_fit\"\n",
    "if not data_folder.exists():\n",
    "    print(\"Warning! The data folder does not exist!\")\n",
    "\n",
    "available_datasets = [\"HERACOMBNCEP920\", \"HERACOMBCCEM\", \"SLACP\", \"HERACOMB_SIGMARED_B\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b6f860",
   "metadata": {},
   "source": [
    "## 1. Prepare the PDF model\n",
    "\n",
    "1. Prepare a PDF model that takes as input `x` and outputs `9` different flavours. You should be able to use what you wrote in the previous tutorial\n",
    "\n",
    "2. Create a layer that rotate a PDF to the evolution basis, in which the fk-tables are generated (as you learn two tutorials ago!)\n",
    "\n",
    "\n",
    "code suggestions:\n",
    "```python\n",
    "\n",
    "# Model building\n",
    "class Preprocessing(tf.keras.layers.Layer):\n",
    "    \"\"\"This layer generates a preprocessing (1-x)**(1+beta)\"\"\"\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"The build function will be called before a forward pass and the trainable weight\n",
    "        will be generated. Beta is constrained to be a positive value to avoid 1/0\"\"\"\n",
    "        self._beta = self.add_weight(\n",
    "            shape=(1,),\n",
    "            trainable=True,\n",
    "            name=\"beta\",\n",
    "            constraint=tf.keras.constraints.non_neg(),\n",
    "            initializer=\"ones\",\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        return (1.0 - x) ** (self._beta + 1.0)\n",
    "\n",
    "\n",
    "class InputScaling(tf.keras.layers.Layer):\n",
    "    \"\"\"This layer applies a logarithmic scaling to the input and then concatenates it to the actual input\n",
    "    This layer is dim=1 --> dim=2\n",
    "    \"\"\"\n",
    "\n",
    "    def call(self, x):\n",
    "        return tf.concat([x, tf.math.log(x)], axis=-1)\n",
    "\n",
    "\n",
    "def generate_pdf_model(outputs=9, units=16, nlayers=4, activation=\"tanh\"):\n",
    "    \"\"\"Generate a PDF model such that\n",
    "    f(x) = (1-x)^beta * NN(x, log(x))\n",
    "    \"\"\"\n",
    "    # Note that we have added a \"None\" size here, we will see in a moment why!\n",
    "    input_layer = tf.keras.layers.Input(shape=(None, 1))\n",
    "    scaled_input = InputScaling()\n",
    "    preprocessing_factor = Preprocessing()\n",
    "    mm_layer = tf.keras.layers.Multiply()\n",
    "\n",
    "    # Prepare the sequential PDF model\n",
    "    pdf_raw = Sequential(name=\"pdf\")\n",
    "    pdf_raw.add(scaled_input)\n",
    "    for _ in range(nlayers):\n",
    "        pdf_raw.add(keras.layers.Dense(units, activation=activation))\n",
    "    pdf_raw.add(keras.layers.Dense(outputs, activation=\"linear\"))\n",
    "\n",
    "    final_result = mm_layer([pdf_raw(input_layer), preprocessing_factor(input_layer)])\n",
    "    return tf.keras.models.Model(input_layer, final_result)\n",
    "\n",
    "\n",
    "pdf_model = generate_pdf_model()\n",
    "test = pdf_model(np.random.rand(2, 20, 1))\n",
    "pdf_model.summary()\n",
    "\n",
    "# Rotation to the evolution basis\n",
    "class EvolutionRotation(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    While the PDFs that we are fitting are in the basis of flavours. Due to the peculiarities of the DGLAP evolution, which is contained in the fktable together with the partonic cross section, it is more convenient to perform the fit in what is known as the \"evolution basis\". And the fktables expect the PDFs to be, indeed in the evolution basis.\n",
    "In this tutorial we are going to keep the fit in the flavour basis but this means we need a rotation from the NN output into the evolution basis.\n",
    "Note that the PDFs we convolute are fitted at a scale of  ùëÑ=1.65  and contain no contribution of the bottom or top quark. In addition we are not consider a photon contribution in this tutorial.\n",
    "\n",
    "    This layer takes a single PDF = NN(x)*(1-x)^beta in the flavour basis and rotates\n",
    "    to the evolution basis\n",
    "\n",
    "                                                       0   1   2   3   4  5  6  7  8\n",
    "    The flavour basis of the NN model in our case is (g, -c, -s, -u, -d, d, u, s, c)\n",
    "    \"\"\"\n",
    "\n",
    "    def call(self, pdf):\n",
    "        # The input pdf has shape (batch, nx, flavours)\n",
    "        pdfT = tf.transpose(pdf)\n",
    "\n",
    "        singlet = tf.reduce_sum(pdfT, axis=0)\n",
    "        g  = pdfT[0]\n",
    "        v = (pdfT[6]-pdfT[3]) + (pdfT[5]-pdfT[4]) + (pdfT[7]-pdfT[2]) + (pdfT[8]-pdfT[1])\n",
    "        v3  = (pdfT[6]-pdfT[3]) - (pdfT[5]-pdfT[4]) \n",
    "        v8  = (pdfT[6]-pdfT[3]) + (pdfT[5]-pdfT[4]) - 2*(pdfT[7]-pdfT[2])\n",
    "        v15 = (pdfT[6]-pdfT[3]) + (pdfT[5]-pdfT[4]) + (pdfT[7]-pdfT[2]) - 3*(pdfT[8]-pdfT[1])\n",
    "        t3  = (pdfT[6]+pdfT[3]) - (pdfT[5]+pdfT[4])\n",
    "        t8  = (pdfT[6]+pdfT[3]) + (pdfT[5]+pdfT[4]) - 2*(pdfT[7]+pdfT[2])\n",
    "        t15 = (pdfT[6]+pdfT[3]) + (pdfT[5]+pdfT[4]) + (pdfT[7]+pdfT[2]) - 3*(pdfT[8]+pdfT[1])\n",
    "\n",
    "        # All other members of the evolution basis contain redundant information at the fitting scale\n",
    "        photon = tf.zeros_like(g)\n",
    "        v24 = v\n",
    "        v35 = v\n",
    "        t24 = singlet\n",
    "        t35 = singlet\n",
    "\n",
    "        pdf_evol_list = [photon, singlet, g, v, v3, v8, v15, v24, v35, t3, t8, t15, t24, t35]\n",
    "        # The output will be (nx, 14)\n",
    "        return tf.concat(pdf_evol_list, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e86da5e-e33d-4229-a584-6e2fa2167446",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_model = generate_pdf_model()\n",
    "\n",
    "# Our PDF model should now be able to take any number of \"datasets\"\n",
    "# for which it will need \"nx\" points to perform the PDF x Cross Section convolution\n",
    "# and each of these \"nx\" point are dimension 1\n",
    "fake_input = np.random.rand(2, 20, 1)\n",
    "\n",
    "test = pdf_model(fake_input)\n",
    "pdf_model.summary()\n",
    "\n",
    "print(f\"Does the PDF model produces the correct output? {np.allclose(test.shape, [2, 20, 9])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2270f2",
   "metadata": {},
   "source": [
    "### 2. Create a trainable model for which the output is the data!\n",
    "\n",
    "In order to use the built-in algorithms in tensorflow for training, we need to write the integral over the `x` in a way that tensorflow can understand.\n",
    "Furthermore, computing the complete integral in a per-epoch or per-event basis would be too computationally expensive.\n",
    "Thanks to the power of the FK-Tables we can make that integral into a convolution, also with TensorFlow!\n",
    "\n",
    "Furthermore, the raining on datasets is a bit more subtle than the simple training with the PDF as the target.\n",
    "\n",
    "When training against a PDF we had a situation in which every point in the input corresponds to a single point in the output, so the loss function (and the model) is a relatively simple one:\n",
    "\n",
    "\\begin{equation}\n",
    "    l(x) = y(x) - t(x)\n",
    "\\end{equation}\n",
    "\n",
    "However, now in order to train against data we need to perform the integral (which we approximate by a convolution). This means that many values of `x` correspond to a single value of the output. And many values of the output are generated by the same input grid.\n",
    "\n",
    "\n",
    "1. Write a \"Convolution\" layer that is constructed taking as input the fktable and the basis of flavours that the fktables uses and computes the predictions to be compared with the experimental data.\n",
    "\n",
    "2. Write a function to compare the results of your fit with the actual experimental data and look at the comparison.\n",
    "\n",
    "3. Write a loss function so that you can train a model which includes a convolution\n",
    "\n",
    "4. Fit against one of the dataset, you choose which.\n",
    "\n",
    "5. Compare the results of your fit with your dataset.\n",
    "\n",
    "6. Compare now the results of your fit for a different dataset! What has happened? \n",
    "\n",
    "code suggestion:\n",
    "```python\n",
    "# On Tuesday you use pineappl to perform the convolution\n",
    "# in this tutorial you will need to write the convolution of the grid and the PDF by yourself!\n",
    "\n",
    "# Step 1 is to select the flavours of the PDF that participate in the convolution\n",
    "    masked_pdf = tf.boolean_mask(pdf, basis, axis=1)\n",
    "# Then you can use einsum to perform the actual convolution\n",
    "    tf.einsum(\"nfx, xf -> n\", fktable, masked_pdf)\n",
    "    \n",
    "# Construct loss function that is able to digest both the output of the model and the experimental data \n",
    "def chi2_simplified(ytrue, ypred):\n",
    "    \"\"\"Loss function to pass to the model\"\"\"\n",
    "    return tf.reduce_sum((ytrue - ypred) ** 2)\n",
    "    \n",
    "# In order to train against data, you can do the following:   \n",
    "dsname = available_datasets[3]\n",
    "data = np.load(data_folder / f\"{dsname}.npz\")\n",
    "# We modify the input so that it takes 3 axis (even if two of them size=1)\n",
    "x = data.get(\"xgrid\").reshape(1, -1, 1)  # 1 batch, N datapoints, n dim\n",
    "lbasis = data.get(\"luminosity\")\n",
    "fktable = data.get(\"fktable\")\n",
    "\n",
    "# We would need to also reshape the experimental data, so that we match through the batch dimension\n",
    "experimental_data = data.get(\"D\").reshape(1, -1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da3ca6a-4f14-4b90-bb18-b659854bc037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Write the convolution layer\n",
    "\n",
    "class Convolution(tf.keras.layers.Layer):\n",
    "    \"\"\"Convolutes as a partonic cross section with a PDF in the evolution basis\n",
    "\n",
    "    This follows the same strategy as you learned in the previous tutorial! \n",
    "    We can approximate quite well the integral by a convolution wiht an interpolation grid\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fktable, basis, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._fktable = tf.constant(fktable, dtype=tf.float32)\n",
    "        # Make the basis of flavours into a boolean mask\n",
    "        basis_mask = np.zeros(14, dtype=bool)\n",
    "        for i in basis:\n",
    "            basis_mask[i] = True\n",
    "        self._basis = tf.constant(basis_mask)\n",
    "\n",
    "    def call(self, pdf):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa7681f-8213-4a87-ac29-da76369565f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Compare the results using your PDF model with the actual data\n",
    "\n",
    "# Wrapper function to compare the predictions of a PDF with the results of a dataset\n",
    "def theory_data_comparison(dsname, pdf):\n",
    "    \"\"\"Compare the predictions of a given pdf with the selected dataset\n",
    "    \n",
    "    Note that in this example we are using the custom layers that we have created as if they were\n",
    "    numpy operations, without creating a model out of them!\n",
    "    \n",
    "    This function assumes that there are two layers, change it accordingly if it is not the case for you\n",
    "        - EvolutionRotation\n",
    "        - Convolution\n",
    "    \"\"\"\n",
    "    # The data is in the `.npz` format which can be loaded by the numpy \"load\" function\n",
    "    data = np.load(data_folder / f\"{dsname}.npz\")\n",
    "    x = data.get(\"xgrid\").reshape(1, -1, 1)\n",
    "    lbasis = data.get(\"luminosity\")\n",
    "    fktable = data.get(\"fktable\")\n",
    "    experimental_data = data.get(\"D\")\n",
    "\n",
    "    # Rotate the PDF (evaluated in the grid in x)\n",
    "    evolution_pdf = EvolutionRotation()(pdf(x))\n",
    "    convolution_layer = Convolution(fktable, lbasis)\n",
    "    \n",
    "    theory_predictions = convolution_layer(evolution_pdf)\n",
    "    idata = np.arange(len(experimental_data)) # we don't have information about the kinematic variable\n",
    "    \n",
    "    plt.title(f\"Theory-data comparison for {dsname}\")\n",
    "        \n",
    "    plt.errorbar(idata, experimental_data, yerr=0.0, fmt=\".\", label=\"Experimental data\")\n",
    "    plt.errorbar(idata, theory_predictions, yerr=0.0, fmt=\"x\", color=\"red\", label=\"Prediction\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Data index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5d9d36-9b4e-47f5-8497-b2bde567473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to train against data, you can do the following:   \n",
    "dsname = available_datasets[3]\n",
    "data = np.load(data_folder / f\"{dsname}.npz\")\n",
    "# We modify the input so that it takes 3 axis (even if two of them size=1)\n",
    "x = data.get(\"xgrid\").reshape(1, -1, 1)  # 1 batch, N datapoints, n dim\n",
    "lbasis = data.get(\"luminosity\")\n",
    "fktable = data.get(\"fktable\")\n",
    "\n",
    "theory_data_comparison(dsname, pdf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61636808-c26f-4546-8a6a-4f7f9f341f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Write a loss function so that you can train a model which includes a convolution\n",
    "def loss_function(ytrue, ypred):\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92782dd2-766e-499f-ae1c-e6b33c3e85ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Now instantiate your model, compile with the loss function above and train it!\n",
    "\n",
    "# Let's construct a model\n",
    "pdf_model = generate_pdf_model()\n",
    "\n",
    "# Build the necessary layers\n",
    "evolution_layer = EvolutionRotation()\n",
    "convolution_layer = Convolution(fktable, lbasis)\n",
    "\n",
    "# And finally prepare the observable model\n",
    "# Note how we use the Sequential model again to build up a model that includes our original PDF\n",
    "# and its output is then passed to the evolution and then convolution layers\n",
    "obs_model = keras.models.Sequential([pdf_model, evolution_layer, convolution_layer])\n",
    "\n",
    "# Now you can compile with our custom loss function...\n",
    "obs_model.compile(keras.optimizers.Nadam(), loss=loss_function)\n",
    "print(\"Training started...\")\n",
    "\n",
    "# And train!\n",
    "history = obs_model.fit(x, experimental_data.reshape(1, -1), epochs=1000, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e2083b-9895-48da-8389-5de5f1d01aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Perform the comparison with the same dataset you trained against\n",
    "\n",
    "theory_data_comparison(dsname, pdf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddb161d-8f6f-42cc-81f6-caf78de15b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Now, select a different dataset and repeat the same comparison (without retraining!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62245d3",
   "metadata": {},
   "source": [
    "### 3. Train on multiple dataset and with experimental errors\n",
    "\n",
    "For that we are going to generate several separated observable models that all use the same pdf models.\n",
    "At the end we will concatenate all such models. At the end we will compare with the same data for HERACOMB_SIGMARED_B.\n",
    "\n",
    "Note that the input grid in x is always the same. This allow us to greatly simplify the model. In general they can be different and this could be treated from the point of view of your model (where each observable will take a different input) or by a preprocessing of the data (adding extra 0s if necessary).\n",
    "\n",
    "1. Train all datasets at the same time. There are two possibilities for this, either concatenating all outputs, or creating a loss function per dataset. Note that, since in either case the PDF model will be a single one, you will always be training the same PDF!\n",
    "2. Check your results.\n",
    "3. (optional) train leaving one of the datasets out, to see how the model generalizes\n",
    "\n",
    "code suggestions:\n",
    "\n",
    "```python\n",
    "\n",
    "# Create an observable per dataset\n",
    "observables = [ ]\n",
    "\n",
    "## If you choose to concatenate\n",
    "# Use the Keras `Concatenate` layer, to create a concatenation of observables\n",
    "final_layer = keras.layers.Concatenate()(observables)\n",
    "# Use said concatenation as the model output, now your output will be a concatenation of all data\n",
    "final_model = keras.models.Model(model_input, final_layer)\n",
    "\n",
    "## If you choose to use a different loss per output\n",
    "chi2_list = []\n",
    "for covmat in covmats:\n",
    "    chi2_list.append(chi2_function)\n",
    "final_model = keras.models.Model(model_input, observables)\n",
    "final_model.compile(keras.optimizers.Nadam(), loss=chi2_list)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16063d9-ff3e-47f3-8631-0b5487c6597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Use the code suggestions to generate a final model (starting form a pdf_model) that is able to fit several datasets at once\n",
    "\n",
    "pdf_model = generate_pdf_model()\n",
    "\n",
    "output_data = []\n",
    "observables = []\n",
    "covmats = []  # Let's save the covmats to use them later!\n",
    "\n",
    "# Let's recover a reference to the initial layer, the information is always available\n",
    "# this will allow us to instantiate our observable model with the same input shape\n",
    "model_input = pdf_model.input\n",
    "\n",
    "for dsname in available_datasets:\n",
    "    data = np.load(data_folder / f\"{dsname}.npz\")\n",
    "    x = data.get(\"xgrid\").reshape(1, -1, 1)\n",
    "    lbasis = data.get(\"luminosity\")\n",
    "    fktable = data.get(\"fktable\")\n",
    "    covmat = data.get(\"covmat\")\n",
    "    experimental_data = data.get(\"D\").reshape(1, -1)\n",
    "\n",
    "    convolution = Convolution(fktable, lbasis)\n",
    "\n",
    "    obs = keras.models.Sequential([pdf_model, evolution_layer, convolution])\n",
    "\n",
    "    observables.append(obs(model_input))\n",
    "    covmats.append(covmat)\n",
    "    output_data.append(experimental_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4025660d-f45a-4f75-9c72-ef184082b919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Perform the fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7714487b-2c3e-4ac0-a378-dafd3c33e32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Check your results against any of the dataset\n",
    "\n",
    "theory_data_comparison(dsname, pdf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a55a5ea-80c4-4e7f-94c3-d881df2017da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. (optional) perform again the fit leaving one dataset out and check the result against that one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43a6c14",
   "metadata": {},
   "source": [
    "### 4. Include experimental errors\n",
    "\n",
    "Let us summarize what we have done up to now: we have create one single PDF model which takes as input a single grid in x and produces a pdf values for this grid in x. Then the results of this PDF are rotated into the evolution basis and convoluted with an interpolation table to produce a physical observable.\n",
    "\n",
    "Note that while all our models utilize the same convolution and rotation, this is not a requirement. Indeed, we could in the same fit include FKTables for DIS and hadronic observables, include extra contribution or physical constraints.\n",
    "\n",
    "For the output of the model, we have considered that all datapoints are created equal and have concatenated the outputs and compared against a concatenation of the experimental results. This is, as well, not a requirement.\n",
    "In the next exercise we need to use a different loss per output (i.e., one per dataset) since each loss will be different due to the covariance matrix!\n",
    "\n",
    "1. Write class such that you can generate a different loss functions per experiment with different data\n",
    "2. Modify the script to compare model and data so that PDF errors are taken into account.\n",
    "3. Fit and check!\n",
    "\n",
    "\n",
    "code suggestion:\n",
    "```python\n",
    "\n",
    "class Chi2:\n",
    "    def __init__(self, covmat):\n",
    "        self._invcovmat = tf.constant(np.linalg.inv(covmat), dtype=tf.float32)\n",
    "        \n",
    "    def __call__(self, ytrue, ypred):\n",
    "        tmp = (ytrue - ypred)\n",
    "        return tf.einsum(\"bi,ij,bj->b\", tmp, self._invcovmat, tmp)\n",
    "\n",
    "\n",
    "# Add error bars to a plot as the diagonal of the covariance matrix\n",
    "errors = np.sqrt(np.diag(data.get(\"covmat\")))\n",
    "plt.errorbar(idata, experimental_data, yerr=errors, fmt=\".\", label=\"Experimental data\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1a39cc-3064-470d-8611-ce773b6e4d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Use the suggestion above to wrap the loss function as a part of a class so it can hold data\n",
    "\n",
    "chi2_list = []\n",
    "for covmat in covmats:\n",
    "    chi2_list.append(Chi2(covmat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bcc51ba-c1dd-4499-a66f-f8d4434661ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Modify the plot function above to take into account the experimental errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78d8abd1-85e5-4eb9-a8b7-8d7d1f5aec52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Refit and recheck!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bf3c34",
   "metadata": {},
   "source": [
    "### 5. Generate PDF errors\n",
    "Finally, we are going to use the Monte Carlo replica method to generate also an error bar for the predictions.\n",
    "\n",
    "The basis of this method, once we have arrived to this point, is actually quite simple. We are going to generate variations of the output data according to the covariance matrix of each dataset. These variations will be random.\n",
    "\n",
    "Then you will train separate PDF models so that each one of them is optimized with a different \"replica\". At the end you can use this set of separate PDF models to generate uncertainties.\n",
    "\n",
    "1. Generate variations of the datasets. Try to do it on your own! Otherwise there's a suggestion below\n",
    "2. Train a number of independent PDF models (i.e., 5)\n",
    "3. Modify the comparison wrapper so that it can also plot uncertainties for the predictions of the model\n",
    "\n",
    "\n",
    "code suggestion\n",
    "\n",
    "```python\n",
    "# let's save all the information that is shared by all the replicas\n",
    "# (note that the PDF model is not among that!)\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Dataset:\n",
    "    name: str\n",
    "    expdata: np.ndarray\n",
    "    covmat: np.ndarray\n",
    "    convolution: Convolution\n",
    "    chi2: Chi2\n",
    "\n",
    "    @property\n",
    "    def ndata(self):\n",
    "        return self.expdata.shape[-1]\n",
    "\n",
    "    def generate_replica(self):\n",
    "        r = -0.5 + np.random.rand(self.ndata)\n",
    "        return self.expdata + np.dot(self.covmat, r).reshape(1, -1)\n",
    "\n",
    "\n",
    "datasets = []\n",
    "\n",
    "for dsname in available_datasets:\n",
    "    data = np.load(data_folder / f\"{dsname}.npz\")\n",
    "    x = data.get(\"xgrid\").reshape(1, -1, 1)\n",
    "    lbasis = data.get(\"luminosity\")\n",
    "    fktable = data.get(\"fktable\")\n",
    "    covmat = data.get(\"covmat\")\n",
    "    edata = data.get(\"D\").reshape(1, -1)\n",
    "\n",
    "    chi2 = Chi2(covmat)\n",
    "    cc = Convolution(fktable, lbasis)\n",
    "    dd = Dataset(dsname, edata, covmat, cc, chi2)\n",
    "\n",
    "    datasets.append(dd)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976bd698-c9bd-4637-a7ac-bd11dcbcae6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Generate variations of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76490d44-cafc-40b0-af62-6e68511130dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Fit each of the variations separately as a separate PDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1e4284-51bf-4a88-9935-2821021f61cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Use the mean value and the variance of the results of the separate PDF models as a measure of the PDF errors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
