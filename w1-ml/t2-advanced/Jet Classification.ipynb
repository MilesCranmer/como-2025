{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83c9831c",
   "metadata": {},
   "source": [
    "_This notebook is part of the material for the [ML Tutorials](https://github.com/NNPDF/como-2025) session._\n",
    "\n",
    "# Jet Classification with CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d91279",
   "metadata": {},
   "source": [
    "In this tutorial we are going to solve a classification problem.\n",
    "\n",
    "This tutorial is partially based on [this tutorial](https://github.com/bmdillon/dlpp-tutorials/blob/main/tutorial-5-jet-classification-with-cnns.ipynb) and [these notes](https://arxiv.org/pdf/2211.01421.pdf) The data has been curated from the top tagging data from [2107.00656](https://arxiv.org/pdf/2107.00656.pdf).\n",
    "\n",
    "Note that in the original tutorial the files used were 1.5 GB (and around 20gb uncompressed) we have made them much smaller to facilitate running the tutorial. Furthermore, the original tutorial used `pytorch`, instead in this one we are using `tensorflow` to be consistent with the other tutorials (and the `nnpdf` codebase) however all concepts are equally applicable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b13ba8",
   "metadata": {},
   "source": [
    "#### Description of the dataset\n",
    "\n",
    "Top tagging dataset. Printout from `pd4ml.TopTagging.print_description()`.\n",
    "\n",
    "    Description:\n",
    "    14 TeV, hadronic tops for signal, QCD djets background, Delphes ATLAS detector card with Pythia. No MPI/pile-up included\n",
    "    Particle-flow entries (produced by Delphes E-flow) have been clustered into anti-kT 0.8 jets in the pT range [550,650].\n",
    "    All top jets are matched to a parton-level top within âˆ†R = 0.8, and to all top decay partons within 0.8. Also,|eta|_jet < 2 has been required.\n",
    "\n",
    "    Ref:\n",
    "    Deep-learned Top Tagging with a Lorentz Layer by A Butter, G Kasieczka, T and M Russell (arXiv: 1707.08966)\n",
    "\n",
    "    Dataset shape:\n",
    "    ~2M events have been stored divided between training (~1.6M) and test (~400k)) and the shape of the dataset is (# of events, 200, 4).\n",
    "    The feature represent the leading 200 jet constituent four-momenta, with zero-padding for jets that have less than 200.\n",
    "    Constituents are sorted by pT, with the highest pT one first.\n",
    "\n",
    "    The second dataset that is included is just a flag \"ttv\" to identify what the event was before the reshaping operated by us. Here a legenda:\n",
    "        0 = training event;\n",
    "        1 = test event;\n",
    "        2 = validation event;\n",
    "\n",
    "    Note that in the current splitting of the dataset, training and validation events have been merged together as a unique training dataset. So for most intents and purposes one should just train the model on the first dataset and omit the second 'ttv' dataset altogether.\n",
    "\n",
    "    The set label are 0 for QCD and 1 for top.\n",
    "    \n",
    "The 4-momenta is written as (E, px, py, pz)\n",
    "\n",
    "Note:\n",
    "It is possible to repeat this exercise by utilizing the bigger version of the dataset to really se the \"power of the Big Data\". For that please follow the instructions in the official [repository](https://github.com/erum-data-idt/pd4ml/blob/main/examples/1_top_plots.ipynb) of `pd4ml`. In the tutorial below we are using a smaller version of it with ~100k events instead of ~2M."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2739cd0f",
   "metadata": {},
   "source": [
    "# Start of the tutorial\n",
    "\n",
    "Let's begin by loading the top tagging data into the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6731142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's import everything we will need and set the folder where you put all your data!\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow import keras\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 10)\n",
    "data_folder = Path(\"data\")\n",
    "if not data_folder.exists():\n",
    "    print(\"Warning! The data folder does not exist!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57c42ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "dataset = np.load(data_folder / \"toptagging.npz\")\n",
    "raw_input_data = dataset[\"x\"] # (nevents, nconstituents, 4momenta)\n",
    "labels = dataset[\"y\"] # 0 for QCD, 1 for top\n",
    "\n",
    "# Cut down the number of events, setting this to `False` could affect your computer memory!\n",
    "if True:\n",
    "    cutdown = int(4e4)\n",
    "    raw_input_data = raw_input_data[:cutdown]\n",
    "    labels = labels[:cutdown]\n",
    "\n",
    "full_ndata = len(labels)\n",
    "training_ndata = int(full_ndata/2)\n",
    "# Take only part of the data for training, the rest will be \"new unseen data\"\n",
    "raw_test_data = raw_input_data[training_ndata:]\n",
    "test_labels = labels[training_ndata:]\n",
    "# We will only go back to these test_arrays at the end\n",
    "# from now on, we only see training data\n",
    "ndata = training_ndata\n",
    "raw_input_data = raw_input_data[:ndata]\n",
    "labels = labels[:ndata]\n",
    "print(f\"We have loaded {ndata} events (and hidden {full_ndata-ndata} away to test our results later!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f94de55",
   "metadata": {},
   "source": [
    "## Preprocessing jet images\n",
    "\n",
    "We are going to treat this problem as an image classification problem.\n",
    "To that end we are going to transform every jet event into an image.\n",
    "\n",
    "We use the 4-momenta to compute $(p_{T}, \\eta, \\phi)$ (transverse momentum, pseudorapidity and azimuthal angle) for every constituent of the jet and then we use that information to cast them into a grid of pixels\n",
    "\n",
    "The following cell contains auxiliary functions modified from [this notebook](https://github.com/bmdillon/dlpp-tutorials/blob/main/tutorial-5-jet-classification-with-cnns.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f12c73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-10\n",
    "warning_shift_eta = 0\n",
    "warning_shift_phi = 0\n",
    "\n",
    "# Set the grid of 180 x 180 pixels\n",
    "XPIXELS = np.arange(-2.6, 2.6, 0.029)\n",
    "YPIXELS = np.arange(-np.pi, np.pi, 0.035)\n",
    "NX = len(XPIXELS)\n",
    "NY = len(YPIXELS)\n",
    "\n",
    "\n",
    "def comp_eta(pt, pz):\n",
    "    \"\"\"Given the transverse and longitudinal momentum, return the pseudorapidity of the particle\n",
    "    The input arrays are expected to be (n_events, n_constituents)\n",
    "    \"\"\"\n",
    "    # Remove values that might introduce instabilities\n",
    "    safe_vals = (np.abs(pt) > epsilon) & (np.abs(pz) > epsilon)\n",
    "    theta = np.arctan(pt[safe_vals] / pz[safe_vals])\n",
    "    theta[theta < 0] += np.pi\n",
    "    etas = np.zeros_like(pt)\n",
    "    etas[safe_vals] = np.log(np.tan(theta / 2))\n",
    "    # Recover the very small not-zero ones\n",
    "    etas[np.abs(pt) <= epsilon] = epsilon\n",
    "    return etas\n",
    "\n",
    "\n",
    "# Calculate the azimuthal angle of pixel entries\n",
    "def comp_phi(px, py):\n",
    "    \"\"\"Compute the azimuthal angle in rad, np.arctan(0,0)=0 -> zero constituents set to -np.pi\n",
    "    The input arrays are expected to be (n_events, n_constituents)\n",
    "    \"\"\"\n",
    "    phis = np.arctan2(py, px)\n",
    "    phis[phis < 0] += 2 * np.pi\n",
    "    phis[phis > 2 * np.pi] -= 2 * np.pi\n",
    "    phis = phis - np.pi\n",
    "    return phis\n",
    "\n",
    "\n",
    "def dot4m(p1s, p2s):\n",
    "    \"\"\"Compute the dot product between two arrays of 4-momenta, input_shape: (nevents, 4)\"\"\"\n",
    "    vec = np.sum(p1s[:, 1:] * p2s[:, 1:], axis=-1)\n",
    "    return p1s[:, 0] * p2s[:, 0] - vec\n",
    "\n",
    "\n",
    "def invariant_mass(jets):\n",
    "    \"\"\"Compute the invariant mass of the jets. The input array is expected to be (n_events, n_constituent, 4)\"\"\"\n",
    "    sum_momenta = np.sum(jets, axis=1)\n",
    "    m2 = dot4m(sum_momenta, sum_momenta)\n",
    "    return np.sqrt(np.maximum(0.0, m2))\n",
    "\n",
    "\n",
    "def preprocessing_jet(x, y, w, warning_threshold=0.7, rotate=True, flip=True):\n",
    "    \"\"\"Preprocessing of the jets eta and phi (x and y) in preparation for the conversion into images.\n",
    "    The weights (w) is the pt.\n",
    "    The input of this function is 1dimensional arrays of n_constituents.\n",
    "    It returns x and y shifted and rotated so that they are ripe to become the coordinate in an image.\n",
    "\n",
    "    The image is rotated so that the principal axis\n",
    "    is vertical.\n",
    "    Method for calculating principal axis (similar to tensor of inertia):\n",
    "    https://en.wikipedia.org/wiki/Image_moment\n",
    "\n",
    "    At the end of the function the image is flipped so that most weights\n",
    "    lay in the (x<0, y>0)-plane\n",
    "    \"\"\"\n",
    "    # Compute the centroid and shift x/y\n",
    "    norm = w.sum(axis=-1)\n",
    "    x_centroid = (x * w).sum(axis=-1) / norm\n",
    "    y_centroid = (y * w).sum(axis=-1) / norm\n",
    "    x = x - x_centroid[:, None]\n",
    "    y = y - y_centroid[:, None]\n",
    "\n",
    "    # The original notebook talks about possible problems\n",
    "    # with the shifting, let's keep track of that just in case\n",
    "    warning_shift_eta = np.count_nonzero(np.abs(x[:, 0]) > warning_threshold)\n",
    "    warning_shift_phi = np.count_nonzero(np.abs(y[:, 0]) > warning_threshold)\n",
    "    if warning_shift_eta != 0 or warning_shift_phi != 0:\n",
    "        print(f\"Problems in the shifts! {warning_shift_eta=}, {warning_shift_phi=}\")\n",
    "\n",
    "    if rotate:\n",
    "        ### Compute rotation\n",
    "        # Covariant matrix, eigenvectors corr. to principal axis\n",
    "        u11 = (x * y * w).sum(axis=-1) / norm\n",
    "        u20 = (x**2 * w).sum(axis=-1) / norm\n",
    "        u02 = (y**2 * w).sum(axis=-1) / norm\n",
    "        cov = np.array([[u20, u11], [u11, u02]]).T  # (nevents, 2, 2)\n",
    "\n",
    "        evals, evecs = np.linalg.eig(cov)\n",
    "\n",
    "        # Sort the eigenvalues\n",
    "        sort_indices = np.argsort(evals, axis=-1)[::-1]\n",
    "        ev_1 = []\n",
    "        ev_2 = []\n",
    "        for ev, ss in zip(evecs, sort_indices):\n",
    "            ev_1.append(ev[..., ss[0]])  # Eigenvector with largest eigenvalue\n",
    "            ev_2.append(ev[..., ss[1]])\n",
    "        ev_1 = np.array(ev_1)\n",
    "        ev_2 = np.array(ev_2)\n",
    "\n",
    "        # Theta to x_asix, arctan2 gives correct angle\n",
    "        theta = np.arctan2(ev_1[:, 0], ev_1[:, 1])\n",
    "\n",
    "        # Rotation, so that princple axis is vertical\n",
    "        # anti-clockwise rotation matrix\n",
    "        rotation = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n",
    "\n",
    "        stacked_xy = np.stack([x, y])\n",
    "        transformed_mat = np.einsum(\"ijn, inc-> njc\", rotation, stacked_xy)\n",
    "        x_rot = transformed_mat[:, 0]\n",
    "        y_rot = transformed_mat[:, 1]\n",
    "    else:\n",
    "        x_rot = x\n",
    "        y_rot = y\n",
    "\n",
    "    if flip:\n",
    "        wxp = np.sum(x_rot > 0, axis=-1)\n",
    "        wxm = np.sum(x_rot < 0, axis=-1)\n",
    "        wyp = np.sum(x_rot > 0, axis=-1)\n",
    "        wym = np.sum(x_rot < 0, axis=-1)\n",
    "\n",
    "        # We want the X (eta) in the negative quadrant\n",
    "        xflip = np.where(wxm > wxp, 1.0, -1.0)\n",
    "        # And the y (phi) in the positive\n",
    "        yflip = np.where(wym < wyp, 1.0, -1.0)\n",
    "\n",
    "        x_rot *= xflip[:, None]\n",
    "        y_rot *= yflip[:, None]\n",
    "\n",
    "    return x_rot, y_rot\n",
    "\n",
    "\n",
    "def pixelize(x, y, w):\n",
    "    \"\"\"Make the (x, y, w) arrays into pixels. To that end,\n",
    "    discretize x and y by rounding (eta=1.3 -> pixel 1, eta=1.6->pixel2)\n",
    "\n",
    "    The output is nevent images of (NX, NY) pixels\n",
    "    \"\"\"\n",
    "    nevents = x.shape[0]\n",
    "    nconsti = x.shape[1]\n",
    "    z_out = np.zeros((nevents, NX, NY))\n",
    "    ## Get the index in the pixel grid for x and y\n",
    "    x_tmp = x[:, None, :] - XPIXELS[None, :, None]\n",
    "    y_tmp = y[:, None, :] - YPIXELS[None, :, None]\n",
    "    x_coord = np.argmin(np.abs(x_tmp), axis=1)\n",
    "    y_coord = np.argmin(np.abs(y_tmp), axis=1)\n",
    "    # Select only the constituents that are _inside_ the grid of pixels\n",
    "    xmin = XPIXELS[0]\n",
    "    xmax = XPIXELS[-1]\n",
    "    ymin = YPIXELS[0]\n",
    "    ymax = YPIXELS[-1]\n",
    "    in_grid = (xmin <= x) & (x <= xmax) & (ymin <= y) & (y <= ymax)\n",
    "    n_coord = np.repeat(np.arange(nevents)[:, None], nconsti, axis=1)\n",
    "    z_out[n_coord[in_grid], x_coord[in_grid], y_coord[in_grid]] = w[in_grid]\n",
    "    return z_out\n",
    "\n",
    "\n",
    "# function to convert the jet to an image\n",
    "def constituents_to_image(\n",
    "    jets, n_constituents=50, rotate=True, flip=True, zoom_in=True, zoom_pixels=40\n",
    "):\n",
    "    \"\"\"Transform the array of constituents of the jet into an image\n",
    "    The input of this function is an array of (nevents, nconstituents, 4)\n",
    "    The output is shape (nevents, NX*NY)\n",
    "    where NX and NY is the number of pixels in the X and Y axis\n",
    "    note that if the zoom is active they will be equal to zoom_pixels\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        jets: array of events (nevents, nconstituents, 4)\n",
    "        n_constituents: int\n",
    "            How many of the constituents should be used (default = 50)\n",
    "        rotate: bool\n",
    "            rotate the image so that the principal axis is in vertical\n",
    "        flip: bool\n",
    "            flip image so that they are in the (x<0, y>0)-plane\n",
    "        zoom_in: bool\n",
    "            zoom in to a smaller number of pixels around the center (default=true)\n",
    "        zoom_pixels: int\n",
    "            how many pixels around the center we actually want (default=40)\n",
    "    \"\"\"\n",
    "    if n_constituents >= (nc := jets.shape[1]):\n",
    "        raise ValueError(f\"You asked to use {n_constituents} but only {nc} are available\")\n",
    "\n",
    "    jets = jets[:, :n_constituents, :]\n",
    "\n",
    "    pxs = jets[:, :, 1]\n",
    "    pys = jets[:, :, 2]\n",
    "    pzs = jets[:, :, 3]\n",
    "\n",
    "    print(\"Calculating pT...\")\n",
    "    weights = pts = np.sqrt(pxs**2 + pys**2)\n",
    "\n",
    "    print(\"Calculating pseudorapidity... \")\n",
    "    etas = comp_eta(pts, pzs)\n",
    "\n",
    "    print(\"Calculating azimuthal angle\")\n",
    "    phis = comp_phi(pxs, pys)\n",
    "    # This weird preshifting of phi is done here\n",
    "    # because of the shifting done in the preprocessing\n",
    "    phis = (phis.T - phis[:, 0]).T\n",
    "    phis[phis < -np.pi] += 2 * np.pi\n",
    "    phis[phis > np.pi] -= 2 * np.pi\n",
    "\n",
    "    print(\"Preprocessing jets\")\n",
    "    x, y = preprocessing_jet(etas, phis, weights, rotate=rotate, flip=flip)\n",
    "\n",
    "    print(\"Pixelixing\")\n",
    "    z_full = pixelize(x, y, weights)\n",
    "\n",
    "    if not zoom_in:\n",
    "        return z_full.reshape(-1, NX * NY)\n",
    "\n",
    "    print(f\"Cropping images to a {zoom_pixels}x{zoom_pixels} grid\")\n",
    "\n",
    "    # zoom in around the center (and normalize)\n",
    "    # Prepare the crop-coordinates\n",
    "    xcrop_m = int(NX / 2 - zoom_pixels / 2)\n",
    "    xcrop_p = int(NX / 2 + zoom_pixels / 2)\n",
    "    ycrop_m = int(NY / 2 - zoom_pixels / 2)\n",
    "    ycrop_p = int(NY / 2 + zoom_pixels / 2)\n",
    "\n",
    "    z_new = z_full[:, xcrop_m:xcrop_p, ycrop_m:ycrop_p]\n",
    "\n",
    "    # Now normalize\n",
    "    z_norm = np.sum(z_new, axis=(1, 2))[:, None, None]\n",
    "    z_new /= z_norm + 1e-10\n",
    "    return z_new.reshape(-1, zoom_pixels**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8f0efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a window size to zoom in in the most important part of the images\n",
    "pixels = 40\n",
    "print(f\" Preparing the training set\")\n",
    "z_train = constituents_to_image(raw_input_data, zoom_pixels=pixels)\n",
    "print(\"\\n> Preparing the test set\")\n",
    "z_test = constituents_to_image(raw_test_data, zoom_pixels=pixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca2862b",
   "metadata": {},
   "source": [
    "### 1. Data visualization\n",
    "\n",
    "Since we are loading the data as images, before feeding them to the Machine Learning algorithms, it would be great to see them as pictures.\n",
    "\n",
    "1. Show the images of the data.\n",
    "2. Perform a bit of \"human learning\"  by looking at an aggregate of the data (for instance, looking at the averages of background and signal)\n",
    "\n",
    "code suggestions:\n",
    "\n",
    "```python\n",
    "\n",
    "# Print 4 pictures\n",
    "for i in range(4):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    n = np.random.randint(ndata) # Take them randomly\n",
    "    # Note that we need to reshape the array into a pixelsXpixels grid\n",
    "    jet = z_train[n].reshape((pixels, pixels))\n",
    "    if labels[n] == 1:\n",
    "        label = \"Signal\"\n",
    "    else:\n",
    "        label = \"Background\"\n",
    "    plt.title(f\"Jet {n} ({label})\")\n",
    "    plt.imshow(jet, cmap=\"gist_heat_r\")\n",
    "\n",
    "    plt.xlabel(r\"$\\eta$\")\n",
    "    plt.ylabel(r\"$\\phi$\")\n",
    "    \n",
    "    ticks = np.linspace(0, 40, 5)\n",
    "    yti = [r\"$\\pi$\", r\"$\\pi/2$\", 0, r\"$-\\pi/2$\", r\"$-\\pi$\"]\n",
    "    xti = [f\"{i:.2}\" for i in np.linspace(-4, 4, 5)]\n",
    "    plt.yticks(ticks, yti)\n",
    "    plt.xticks(ticks, xti)\n",
    "           \n",
    "# Separate signal from background\n",
    "signal = z_train[ labels==1 ]\n",
    "background = z_train[ labels==0 ]\n",
    "           \n",
    "# Plot the aggreate of signal and background as separate plots\n",
    "mean_signal = signal.mean(axis=0).reshape(pixels, pixels)\n",
    "mean_bkg = background.mean(axis=0).reshape(pixels, pixels)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Signal\")\n",
    "plt.imshow(mean_signal, cmap=\"gist_heat_r\")\n",
    "plt.xlabel(r\"$\\eta$\")\n",
    "plt.ylabel(r\"$\\phi$\")\n",
    "\n",
    "ticks = np.linspace(0, 40, 5)\n",
    "yti = [r\"$\\pi$\", r\"$\\pi/2$\", 0, r\"$-\\pi/2$\", r\"$-\\pi$\"]\n",
    "xti = [f\"{i:.2}\" for i in np.linspace(-4, 4, 5)]\n",
    "plt.yticks(ticks, yti)\n",
    "plt.xticks(ticks, xti)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Background\")\n",
    "plt.imshow(mean_bkg, cmap=\"gist_heat_r\")\n",
    "plt.xlabel(r\"$\\eta$\")\n",
    "plt.ylabel(r\"$\\phi$\")\n",
    "\n",
    "ticks = np.linspace(0, 40, 5)\n",
    "yti = [r\"$\\pi$\", r\"$\\pi/2$\", 0, r\"$-\\pi/2$\", r\"$-\\pi$\"]\n",
    "xti = [f\"{i:.2}\" for i in np.linspace(-4, 4, 5)]\n",
    "plt.yticks(ticks, yti)\n",
    "plt.xticks(ticks, xti);               \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb29316",
   "metadata": {},
   "outputs": [],
   "source": [
    "#....#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d82307",
   "metadata": {},
   "source": [
    "### 2. Building and train  a CNN to clasify data\n",
    "\n",
    "Since we have images, we are going to build a Convolutional Neural Network (hint, use Tensorflow's `sequential` model with the `Conv2D` and `MaxPooling2D` layers).\n",
    "\n",
    "1. Build a NN able to take in the images and return a probability between 0 and 1 of the result beeing background or signal (you can use as a final layer `sigmoid` for that.\n",
    "2. Use the code below to prepare checkpoints with the weights in an epoch by epoch basis.\n",
    "3. Train the network using the checkpoint during the fit\n",
    "5. Plot a training Vs validation curve (note that the test data is not the same as validation data)\n",
    "\n",
    "code suggestions\n",
    "\n",
    "```python\n",
    "\n",
    "# Let's prepare a checkpoint so that we have the best weights of every epoch\n",
    "# this will allow us to play some interesting games\n",
    "# Some more information about this https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint\n",
    "# Let's save the per-epoch weight:\n",
    "output_path = \"output_jets/output_path_epoch_{epoch:03d}.weights.h5\"\n",
    "Path(output_path).parent.mkdir(exist_ok=True)\n",
    "\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint(output_path, \n",
    "                                                   save_best_only=True,\n",
    "                                                   save_weights_only = True,\n",
    "                                                   save_freq=\"epoch\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251772b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#....#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e3423a",
   "metadata": {},
   "source": [
    "### 3. Use the test data to analyze the results\n",
    "\n",
    "At the beginning of the notebook we separated the input data into a training and test sets. We are now going to recover the test set to evaluate the accuracy of the model.\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{accuracy} = 100 \\frac{\\text{correct}}{\\text{total}}\n",
    "\\end{equation}\n",
    "\n",
    "Our model returns a probability for the input to truly be a top jet so we are going to plot a few curves.\n",
    "We will start with a curve of the accuracy as a function of the threshold at which the events get catgorized as signal or background.\n",
    "\n",
    "And then we will plot what are usually known as ROC curve which is the `True Positive Rate` vs. the `False Positive Rate`.\n",
    "\n",
    "The true positive rate is often also called \"signal efficiency\" and it's defined as:\n",
    "\\begin{equation}\n",
    "    \\text{TPR} = \\frac{\\text{TP}}{\\text{TP + FN}}\n",
    "\\end{equation}\n",
    "\n",
    "While the rate of false positives is instead called background rejection:\n",
    "\\begin{equation}\n",
    "    \\text{FPR} = \\frac{\\text{FP}}{\\text{FP + TN}}\n",
    "\\end{equation}\n",
    "\n",
    "1. Plot the ROC curve for different values of the threshold to decide whether an event is data or background.\n",
    "2. Try with the result at different epochs to see how it changed during the fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67b5701",
   "metadata": {},
   "outputs": [],
   "source": [
    "#....#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_como",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
