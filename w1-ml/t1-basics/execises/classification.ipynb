{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3DR-eO17geWu"
      },
      "source": [
        "_This notebook is part of the material for the [ML Tutorials](https://github.com/NNPDF/como-2025) session._\n",
        "\n",
        "# Convolutional Neural Network for Classification\n",
        "\n",
        "In this exercise, we are building a convolutional neural network that learns to distinguish cat and dog images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EMefrVPCg-60"
      },
      "source": [
        "### Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "sCV30xyVhFbE"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oxQxCBWyoGPE"
      },
      "source": [
        "## Part 1 - Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MvE-heJNo3GG"
      },
      "source": [
        "### Preprocessing the Training set\n",
        "\n",
        "To have the images in an appropriate format, we need to preprocess them. A common way to do so is to first scale the pixel values down by 255 to bring them into the range $[0, 1]$. Moreover, we want to ensure that all images have the same size, so we map everything to 64x64 pixels. Then, as we know, when rotating, moving, flipping, or zooming into images of cats and dogs, the cat remains a cat, and a dog remains a dog. In other words, the class definition should be invariant under these transformations.\n",
        "\n",
        "By construction, convolutional neural networks are already invariant under translations, so we do not have to bother about this in the preprocessing. However, to ensure the network learns that the same image, when flipped, rotated, or zoomed, remains in the same class, we typically perform data augmentation. This means you take an image, perform a random flip, rotation, or zoom, and then add the same image with the same label to the dataset. With this, the network effectively learns that these transformations are symmetries of the problem.\n",
        "\n",
        "Of course, in many cases, it is also possible to directly embed symmetries into the network architectures, which allows us to drop the need to augment images that have been additionally transformed and add them to the training set. Here, we do not have such an architecture available, which means we must perform these transformations manually.\n",
        "\n",
        "Luckily, there exists a Keras class, which does this automatically for you. It is called `ImageDataGenerator` ([Doc](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator)) which we will just use in the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "0koUcJMJpEBD"
      },
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "training_set = train_datagen.flow_from_directory('dataset/catdogs/training_set',\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'binary')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mrCMmGw9pHys"
      },
      "source": [
        "### Preprocessing the Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "SH4WzfOhpKc3"
      },
      "outputs": [],
      "source": [
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "test_set = test_datagen.flow_from_directory('dataset/catdogs/test_set',\n",
        "                                            target_size = (64, 64),\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode = 'binary')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "af8O4l90gk7B"
      },
      "source": [
        "## Part 2 - Building the CNN\n",
        "\n",
        "Needed parts:\n",
        "- You can either use the [Functional API](https://keras.io/guides/functional_api/) or the [sequential model](https://keras.io/guides/sequential_model/)\n",
        "- Input layer: needed to make the network aware the format of the dataset. Here it is `[64,64,3]`, as we have preprocessed all images to `64x64` pixels and it consists of 3 (color) channels.\n",
        "- Add combination of [convolutions](https://keras.io/api/layers/convolution_layers/) and [pooling](https://keras.io/api/layers/pooling_layers/) layers.\n",
        "- [Flatten](https://keras.io/api/layers/reshaping_layers/flatten/) + [Fully Connected](https://keras.io/api/layers/core_layers/dense/) layers\n",
        "- Output layer with apropriate activation function\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D6XkI90snSDl"
      },
      "source": [
        "## Part 3 - Training the CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vfrFQACEnc6i"
      },
      "source": [
        "### Compiling the CNN\n",
        "\n",
        "Check the [model.compile](https://keras.io/api/models/model_training_apis/#compile-method) function.  \n",
        "Use Adam optimizer and Binary-Crossentropy Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "NALksrNQpUlJ"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ehS-v3MIpX2h"
      },
      "source": [
        "### Training the CNN on the training set and evaluating it on the test set\n",
        "\n",
        "Use the Keras built-in [model.fit](https://keras.io/api/models/model_training_apis/#fit-method) function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "XUj1W4PJptta"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "U3PZasO0006Z"
      },
      "source": [
        "## Part 4 - Making a single prediction\n",
        "\n",
        "Check your model on the sinlge pictures in `dataset/single_prediction` whether it predicts the correct\n",
        "classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "gsSiWEJY1BPB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "test_image = image.load_img('dataset/single_prediction/cat_or_dog_1.jpg', target_size = (64, 64))\n",
        "test_image = image.img_to_array(test_image)/255.0 # <-rescale\n",
        "test_image = np.expand_dims(test_image, axis = 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Make a prediction with the trained model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ED9KB3I54c1i"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "convolutional_neural_network.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "env_como",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
