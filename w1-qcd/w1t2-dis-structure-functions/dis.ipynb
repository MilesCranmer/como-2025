{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f53200127974cb9",
   "metadata": {},
   "source": [
    "# Tutorial 2 - DIS structure functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429f8345-43b9-47d2-bb85-848d5d758bcd",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pineappl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e20b061-4a01-4b79-9d7c-01d535380d6c",
   "metadata": {},
   "source": [
    "In this second tutorial, we are going to build a numerical implementation of the DIS coefficient functions in order to compare to experimental data. We consider the following DIS process,\n",
    "$$ e^-(l) + p(P) \\to e^-(l') + X $$\n",
    "and we define the usual DIS variables:\n",
    "$$ q = l - l', \\quad Q^2 = - q^2, \\quad x = \\frac{Q^2}{2q\\cdot P} $$\n",
    "where $q$ is the momentum transfer, $Q^2$ the virtuality of the intermediate photon/gauge boson and $x$ the BjÃ¶rken x variable.\n",
    "A common observable to look at is the reduced cross section\n",
    "$$ \\sigma_{red}(x,Q^2) = \\mathcal N \\left\\langle \\frac{d^2\\sigma^{pe^- \\to e^-+X}}{dx\\, dQ^2} \\right\\rangle \\approx F_2(x,Q^2) $$\n",
    "with some normalization $\\mathcal N$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa2780c-d27b-41cf-9249-38d0395450eb",
   "metadata": {},
   "source": [
    "Along similar lines as explained in the first tutorial, we find at NLO accuracy\n",
    "$$\n",
    "  \\frac 1 x F_2(x,Q^2) = \\sum_a (C_a \\otimes f_a^p)(x,Q^2) = \\sum_a \\int\\limits_x^1 \\frac{dz}{z} C_a(z,Q^2) f_a^p(x/z,Q^2)  = \\sum_a\\sum_{n=0} \\left(\\frac{\\alpha_s(Q^2)}{4\\pi}\\right)^n \\int\\limits_x^1 \\frac{dz}{z} C_a^{(n)}(z) f_a^p(x/z,Q^2)\n",
    "$$\n",
    "with $C_a(z) = e_a^2 c_q(z)$ for the quark channels, i.e. $a=u,\\bar u,d,\\bar d,s,\\bar s,\\ldots$,\n",
    "and $C_g(z) = \\langle e^2\\rangle c_g(z)$ for the gluon channel, with the average squared charge $\\langle e^2\\rangle$.\n",
    "The raw coefficient functions are given by\n",
    "$$ c_q^{(0)}(z) = \\delta(1-z) \\quad c_g^{(0)}(z) = 0 $$\n",
    "$$ c_q^{(1)}(1) = C_F \\left( 4 \\left(\\frac{\\ln(1-z)}{1-z}\\right)_+ - 3 \\left(\\frac{1}{1-z}\\right)_+ - (6+4\\zeta(2))\\delta(1-z) -2(1+z)\\ln((1-z)/z) -4 \\frac{\\ln(z)}{1-z} +6 +4z \\right)  $$\n",
    "$$ c_g^{(1)}(z) = 2 T_R n_f \\left((2-4z(1-z)\\ln((1-z)/z)) -2 +16z(1-z)\\right) $$\n",
    "\n",
    "*Further Reading:* you want to learn more about DIS coefficient functions? The N3LO results are available [here](https://inspirehep.net/literature/681335)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e93a5c-3cbe-409d-a205-75be30bc8b2a",
   "metadata": {},
   "source": [
    "The plus distribution, related to soft and collinear divergences, is defined as\n",
    "$$\\int\\limits_0^1 dz g(z) (c(z))_+ = \\int\\limits_0^1 dz (g(z) - g(1)) c(z) \\quad$$\n",
    "and note that this form does not coincide with the factorization formula (note the integration limits!) - we thus need to find a different representation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e5a5a6-e901-4737-9855-b16bf6f2324b",
   "metadata": {},
   "source": [
    "Here, a suitable representation is the \"RSL scheme\" where we divide any coefficient function $c$ in a **R**egular, **S**ingular, and **L**ocal part defined by their behavior under the convolution integral\n",
    "$$ (c\\otimes f)(x) = \\int\\limits_x^1 \\frac{dz}{z} f(x/z) c^R(z) +  \\int\\limits_x^1 dz\\left(\\frac{f(x/z)}{z} - f(x)\\right)  c^S(z) + f(x)c^L(x)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281ea052-7f38-40ef-a3ec-c4ad17e90a70",
   "metadata": {},
   "source": [
    "### Excercise 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0880e33-c766-43e1-8ee2-3f849bb7d364",
   "metadata": {},
   "source": [
    "As an exercise with the RSL scheme, verify that\n",
    "$$ c(z) = \\left(\\frac{\\ln(1-z)}{1-z}\\right)_+ \\Leftrightarrow c^R(z) = 0, c^S(z)= \\frac{\\ln(1-z)}{1-z}, c^L(x) = \\frac{\\ln^2(1-x)}{2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d63ecb8-4f83-4f21-90c3-ad70b4c4dadd",
   "metadata": {},
   "source": [
    "### NLO DIS coefficient functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb6a594-336d-4448-90fe-625586ba0dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's fix some global constants first\n",
    "CF = 4.0 / 3.0  # Casimir constant in fundamental color representation\n",
    "TR = 1.0 / 2.0  # Quark commutator normalization\n",
    "e2u = 4.0 / 9.0  # squared electric charge of up-like quarks\n",
    "e2d = 1.0 / 9.0  # squared electric charge of down-like quarks\n",
    "nf = 5  # number of active (light) flavors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f221d762-0e4a-4455-b002-4876e7dc4f07",
   "metadata": {},
   "source": [
    "Most of the numbers may look familiar to you - we will comment in the next section about the number of light flavors.\n",
    "\n",
    "Now, we can give the equivalent RSL representation of the NLO DIS coefficient functions, implemented in the following python functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a850e863-26e4-4735-b1fa-82afdf312f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import zeta\n",
    "from dataclasses import dataclass\n",
    "from collections.abc import Callable\n",
    "\n",
    "\n",
    "# Let's define a helper class to collect all ingredients to the RSL representation: three functions\n",
    "@dataclass(frozen=True)\n",
    "class RSL:\n",
    "    regular: Callable[[float], float]\n",
    "    singular: Callable[[float], float]\n",
    "    local: Callable[[float], float]\n",
    "\n",
    "\n",
    "# we can now spell out all coefficient functions\n",
    "# LO quark\n",
    "def cq_LO_R(z: float) -> float:\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def cq_LO_S(z: float) -> float:\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def cq_LO_L(x: float) -> float:\n",
    "    return 1.0\n",
    "\n",
    "\n",
    "cq_LO = RSL(regular=cq_LO_R, singular=cq_LO_S, local=cq_LO_L)\n",
    "\n",
    "\n",
    "# NLO quark\n",
    "def cq_NLO_R(z: float) -> float:\n",
    "    return CF * (\n",
    "        -2 * (1 + z) * np.log((1 - z) / z) - 4 * np.log(z) / (1 - z) + 6 + 4 * z\n",
    "    )\n",
    "\n",
    "\n",
    "def cq_NLO_S(z: float) -> float:\n",
    "    return CF * (4.0 * np.log(1.0 - z) - 3.0) / (1.0 - z)\n",
    "\n",
    "\n",
    "def cq_NLO_L(x: float) -> float:\n",
    "    return CF * (\n",
    "        4.0 * np.log(1.0 - x) ** 2.0 / 2.0\n",
    "        - 3.0 * np.log(1.0 - x)\n",
    "        - (9.0 + 4.0 * zeta(2))\n",
    "    )\n",
    "\n",
    "\n",
    "cq_NLO = RSL(regular=cq_NLO_R, singular=cq_NLO_S, local=cq_NLO_L)\n",
    "\n",
    "\n",
    "# NLO gluon\n",
    "def cg_NLO_R(z: float) -> float:\n",
    "    return (\n",
    "        (2.0 * TR)\n",
    "        * nf\n",
    "        * (\n",
    "            (2.0 - 4.0 * z * (1.0 - z)) * np.log((1.0 - z) / z)\n",
    "            - 2.0\n",
    "            + 16.0 * z * (1.0 - z)\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def cg_NLO_S(z: float) -> float:\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def cg_NLO_L(x: float) -> float:\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "cg_NLO = RSL(regular=cg_NLO_R, singular=cg_NLO_S, local=cg_NLO_L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160bcfbf-c08e-446e-9e5e-ac0b1c18b3ec",
   "metadata": {},
   "source": [
    "Okay, now we have all necessary formulae and codes - let's do it! Lets implement a simple DIS code using the DIS factorization formula and the coefficient functions from above.\n",
    "\n",
    "Actually, for the sake of this tutorial, let's *not* do this in one big step, but in three smaller steps. By the way, this is a good advice for programming: if you can break it in smaller steps, you should break it in smaller steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce07cce-5b64-48b5-9ee7-1bc6f3d100e7",
   "metadata": {},
   "source": [
    "### Excercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3c2450-4773-487f-9584-f30143ff2a5a",
   "metadata": {},
   "source": [
    "Let's abstract the task of convoluting a RSL coefficient function with another function first.\n",
    "\n",
    "Here you should write a function to convolute an `RSL` object with an other function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40a99e8-855c-47a6-9734-ee866154a6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import quad, IntegrationWarning\n",
    "\n",
    "\n",
    "def convolute(c: RSL, f: Callable[[float], float], x: float) -> float:\n",
    "    r\"\"\"Convolute c(z) with f(z) with respect to x: :math:`(c\\otimes f)(x)`.\"\"\"\n",
    "    \n",
    "    # write your code here ...\n",
    "    # recall the RSL definition from above!\n",
    "    # at some point you want to use quad for the integration ...\n",
    "    # see https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.quad.html#scipy.integrate.quad\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7156ff8d51fae3",
   "metadata": {},
   "source": [
    "You can check your implementation by running the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103b54b6-7b19-4b4e-a050-3806fadd885e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your implementation here!\n",
    "np.testing.assert_allclose(\n",
    "    convolute(\n",
    "        RSL(regular=lambda _z: 1.0, singular=lambda _z: 0.0, local=lambda _x: 0.0),\n",
    "        lambda z: z * (1.0 - z),\n",
    "        0.5,\n",
    "    ),\n",
    "    0.125,\n",
    ")\n",
    "np.testing.assert_allclose(\n",
    "    convolute(\n",
    "        RSL(regular=lambda z: z, singular=lambda _z: 0.0, local=lambda _x: 0.0),\n",
    "        lambda z: z * (1.0 - z),\n",
    "        0.5,\n",
    "    ),\n",
    "    0.09657359027972266,\n",
    ")\n",
    "np.testing.assert_allclose(\n",
    "    convolute(\n",
    "        RSL(regular=lambda _z: 0.0, singular=lambda _z: 0.0, local=lambda _x: 1.0),\n",
    "        lambda z: z * (1.0 - z),\n",
    "        0.5,\n",
    "    ),\n",
    "    0.25,\n",
    ")\n",
    "np.testing.assert_allclose(\n",
    "    convolute(\n",
    "        RSL(\n",
    "            regular=lambda _z: 0.0,\n",
    "            singular=lambda z: 1.0 / (1.0 - z),\n",
    "            local=lambda _x: 0,\n",
    "        ),\n",
    "        lambda z: z * (1.0 - z),\n",
    "        0.5,\n",
    "    ),\n",
    "    0.04828679513973628,\n",
    ")\n",
    "np.testing.assert_allclose(\n",
    "    convolute(\n",
    "        RSL(\n",
    "            regular=lambda z: z,\n",
    "            singular=lambda z: 1.0 / (1.0 - z),\n",
    "            local=lambda x: np.log(1.0 - x),\n",
    "        ),\n",
    "        lambda z: z * (1.0 - z),\n",
    "        0.5,\n",
    "    ),\n",
    "    -0.0284264097205274,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e83dedb-a5cb-4f81-9e30-f79e2bc3bd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have a small mathematical hiccup in our NLO quark coefficient function:\n",
    "# a removable singularity at z = 1 through log(z)/(1-z) .\n",
    "# For details see also https://en.wikipedia.org/wiki/Removable_singularity\n",
    "np.testing.assert_allclose(\n",
    "    convolute(\n",
    "        RSL(\n",
    "            regular=lambda z: np.log(z) / (1.0 - z),\n",
    "            singular=lambda _z: 0.0,\n",
    "            local=lambda _x: 0.0,\n",
    "        ),\n",
    "        lambda z: z * (1.0 - z),\n",
    "        0.5,\n",
    "    ),\n",
    "    -0.1431167583560283,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3047b3e-cbff-4e9c-9ac3-cec3f05b1a6d",
   "metadata": {},
   "source": [
    "### Excercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057fc91a-2740-4d6c-b88f-fd3ae48d2397",
   "metadata": {},
   "source": [
    "Next, let's take a look at the flavor structure: in our master formula we wrote\n",
    "$$ \\frac 1 x F_2(x,Q^2) = \\sum_{a} C_a \\otimes f_a^p $$\n",
    "where the sum $a$ runs over all the 5 light quarks ($d,\\bar d, u, \\bar u, s,\\bar s, c, \\bar c, b, \\bar b$) and the gluon $g$ (so 11 elements).\n",
    "However, at NLO there are actually only 2 different type of input configurations possible: a quark or a gluon.\n",
    "The information which quark is entering is only needed for the coupling (i.e. the respective electric charge) which is a mere prefactor\n",
    "and in a similar way we can factorize for the gluon channel the information on how many quarks are coupling to the photon.\n",
    "This means at NLO we can write\n",
    "$$ \\frac 1 x F_2(x,Q^2) = \\sum_{b=\\{q,g\\}} c_b \\otimes l_b^p $$\n",
    "where we refer to the functions $l_b$ as luminosity channel. \n",
    "\n",
    "Define the two luminosity functions $l_q^p(x,Q^2)$ and $l_g^p(x,Q^2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89e9347-f56a-4d53-90b2-cac91c4ca35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lhapdf\n",
    "\n",
    "# Let's use NNPDF4.0 - so we load it via LHAPDF\n",
    "pdf = lhapdf.mkPDF(\"NNPDF40_nnlo_as_01180\", 0)\n",
    "# recall that you can access PDFs via\n",
    "xgluon_at_x0p5_Q2100GeV2 = pdf.xfxQ2(21, 0.5, 100.0)  # = x*g(x=0.5,Q^2=100 GeV^2)\n",
    "# where the function signature of xfxQ2 is pid,x,Q^2\n",
    "# and the pid of the gluon is 21 while it is 1=d,-1=dbar,2=u, etc. see also https://pdg.lbl.gov/2022/reviews/contents_sports.html\n",
    "# also recall that xfxQ2 returns x * PDF, but in our factorization formula we need the true PDF\n",
    "\n",
    "\n",
    "# recall the deifitions of C and c from above ...\n",
    "def lumi_q(x: float, Q2: float) -> float:\n",
    "    \"\"\"Quark luminosity at momentum fraction x and factorization scale Q2.\"\"\"\n",
    "    # write your code here ...\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def lumi_g(x: float, Q2: float) -> float:\n",
    "    \"\"\"Gluon luminosity at momentum fraction x and factorization scale Q2.\"\"\"\n",
    "    # write your code here ...\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af609e2a-ebbc-4517-81a2-19eedf244430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your implementation here!\n",
    "np.testing.assert_allclose(lumi_q(0.1, 10.0), 4.294867071979338)\n",
    "np.testing.assert_allclose(lumi_g(0.1, 10.0), 3.2411771900340334)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8146df-1da7-413c-8d43-31d74689447c",
   "metadata": {},
   "source": [
    "Knowing and identifying the luminosity structure of an observable is important.\n",
    "This way, we can try to find new observables which expose a certain luminosity structure to access different physics related to different flavors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb698ebf-a535-4b8f-be1a-77084b687d36",
   "metadata": {},
   "source": [
    "### Excercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9944da2c-353f-4de1-bee0-60f3707ae60a",
   "metadata": {},
   "source": [
    "Finally, we can put everything together.\n",
    "\n",
    "Using your results from the previous exercises, write a function to compute a DIS structure function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1f2c02-d830-4073-9aa4-b51bd365ba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall that you can access the strong coupling via\n",
    "alpha_s_at_100GeV2 = pdf.alphasQ2(100.0)\n",
    "\n",
    "\n",
    "def f2(x: float, Q2: float) -> float:\n",
    "    \"\"\"DIS F2 structure function at Bjorken x and virtuality Q2.\"\"\"\n",
    "    # write your code here ...\n",
    "    # make use of exercise 1 and 2\n",
    "    # also recall the normalization of the lhs of our factorization formula\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784b2767-e9f0-4497-aaae-2c11c60fb222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check you implementation here!\n",
    "# if you want to start with a leading order comparison (which is a good idea!), uncomment the following line\n",
    "# np.testing.assert_allclose(f2(0.001, 10), 0.9948344436236175, rtol=3e-4)\n",
    "# else, we want of course the most precise calculation! (we will compare to real data in the next subsection)\n",
    "# (Again, let's not worry about IntegrationWarnings for the moment ...)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=IntegrationWarning)\n",
    "    np.testing.assert_allclose(f2(0.001, 10), 1.0189, rtol=3e-4)\n",
    "    np.testing.assert_allclose(f2(0.001, 1000), 2.5299, rtol=3e-4)\n",
    "    np.testing.assert_allclose(f2(0.1, 100), 0.4023, rtol=3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f62bea-37d1-4ce8-97ba-c8e41b6ebd80",
   "metadata": {},
   "source": [
    "## Comparing to data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b868b8-2f0d-489c-97c0-81d2cbfb4c71",
   "metadata": {},
   "source": [
    "Now, we are ready to confront our theoretical prediction with some real life experimental data:\n",
    "DIS was and is a major key stone in determining PDFs. Some of the most influential datasets are coming from\n",
    "[HERA](https://en.wikipedia.org/wiki/HERA_(particle_accelerator)) which operated at [DESY](https://en.wikipedia.org/wiki/DESY) in Germany between 1992 and 2007 (there is some data still published nowadays!).\n",
    "Here, we use the combined measurement of the two experiments [H1](https://en.wikipedia.org/wiki/H1_(particle_detector)) and [ZEUS](https://en.wikipedia.org/wiki/ZEUS_(particle_detector)) for the collision of an electron with an proton at a center of mass energy of $\\sqrt{s} = 318\\,\\text{GeV}$ available from [hepdata](https://doi.org/10.17182/hepdata.68951.v1/t5) or the [HERA website](https://www.desy.de/h1zeus/herapdf20/) (in practice, here, we use the latter).\n",
    "\n",
    "This data is also used inside the NNPDF PDF determination, so we should better find some agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8337d805-b90f-4083-999d-7fea715e53d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the experimental data via pandas\n",
    "data = pd.read_csv(\"./HERA1+2_NCem.dat\", sep=\"\\\\s+\")\n",
    "# the (for us) relevant columns are Q2, x and Sigma, which is the actual measurement\n",
    "pd.concat([data[\"Q2\"], data[\"x\"], data[\"Sigma\"]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7478708f-488f-4b11-8720-0b7747df302f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's compute our own predictions for the given points!\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=IntegrationWarning)\n",
    "    f2_data = data.apply(\n",
    "        lambda dat: pd.Series(\n",
    "            # we can keep the name 'Sigma' for the experimental value and just add a column 'F2' for our numbers\n",
    "            [dat[\"x\"], dat[\"Q2\"], dat[\"Sigma\"], f2(dat[\"x\"], dat[\"Q2\"])],\n",
    "            index=[\"x\", \"Q2\", \"Sigma\", \"F2\"],\n",
    "        ),\n",
    "        axis=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0745483c5006a4",
   "metadata": {},
   "source": [
    "Now, lets plot our predictions together with the experimental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98b140f-a77c-4e12-8cb9-891e239cded6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# let's plot the experimental data against our predictions!\n",
    "plt.close()\n",
    "fig, axes = plt.subplots(6, 4, figsize=(10, 15), clear=True, layout=\"tight\")\n",
    "fig.suptitle(r\"HERA $e^-\\! + p \\,\\to\\, e^-\\! + X$ with $\\sqrt{s}=318 \\mathrm{GeV}$\")\n",
    "# the data is organised by Q2 - so let's make a plot for each value\n",
    "for q2, ax in zip(f2_data[\"Q2\"].unique(), axes.flatten()):\n",
    "    # select the correct data\n",
    "    dat = f2_data[f2_data[\"Q2\"] == q2]\n",
    "    ax.set_title(f\"$Q^2 = {q2}\\\\,\\\\mathrm{{GeV}}^2$\")\n",
    "    # and plot\n",
    "    (pexp,) = ax.plot(dat[\"x\"], dat[\"Sigma\"], \"o\", label=\"exp\")\n",
    "    (pth,) = ax.plot(dat[\"x\"], dat[\"F2\"], \"x\", label=\"th\")\n",
    "    # it is convenient to plot log(x) instead of just x\n",
    "    ax.set_xscale(\"log\")\n",
    "fig.legend(handles=[pexp, pth], ncols=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8c2c96-d982-42af-9f5f-cc200514d2c3",
   "metadata": {},
   "source": [
    "Using a simple NLO calculation we can already see that we can get some decent agreement between theory and experiment which can give us some confidence that something is working. However, we also see that we do not match everywhere - so, instead of saying \"a simple NLO calculation\" we should say \"a naive NLO calculation\", because the full story behind DIS is much more complicated! This, we will discuss next. Note that we are dealing with neither theoretical nor experimental uncertainties here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f9a086-71a2-4abf-bf1a-1ffba5fe41df",
   "metadata": {},
   "source": [
    "# Computing more complete DIS predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e14fd5-2540-4508-beec-42b4a08eed7b",
   "metadata": {},
   "source": [
    "As we said above, our calculation so far is fine for some points but a bit too simplistic and the full picture to compare with actual DIS\n",
    "measurements is significantly more complex. We develop the [yadism](https://github.com/NNPDF/yadism) package to deal with these problems and next we want to hightlight some of these complications. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4e782c-bda9-4e8d-9ff5-ac8f41b9fe75",
   "metadata": {},
   "source": [
    "## More leptons, hadrons, exchanged bosons and more structure functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6887c18-e6e9-4b3d-94bb-277acd2536db",
   "metadata": {},
   "source": [
    "In our calculation above we assumed that an electron scatters of an proton with a virtual photon being exchanged and we implicitly assumed the photon has no longitudinal polarization - in yadism we can lift any of these assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62905791-373f-4407-aa62-0d8c3d54cb68",
   "metadata": {},
   "source": [
    "Available leptons are the electron and the positron (as e.g. used at [HERA](https://en.wikipedia.org/wiki/HERA_(particle_accelerator))), but also\n",
    "the neutrino and the anti-neutrino (as e.g. used at NuTeV at [Fermilab](https://en.wikipedia.org/wiki/Fermilab)).\n",
    "However, since it is experimentally very challenging to control a neutrino beam, these are usually indirectly produced (see e.g. the [FPF proposal at CERN](https://arxiv.org/abs/2109.10905))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da57a441-4f1b-4189-a6a4-12a2013c345c",
   "metadata": {},
   "source": [
    "We can use all types of hadrons: the most common target is, of course, protons (as e.g. used at [HERA](https://en.wikipedia.org/wiki/HERA_(particle_accelerator))), but also deuteron targets (as e.g. used at [SLAC](https://en.wikipedia.org/wiki/SLAC_National_Accelerator_Laboratory)) or nuclear targets, such as lead (as e.g. used at NuTeV at [Fermilab](https://en.wikipedia.org/wiki/Fermilab)). Especially the details on the internal structure of nuclei is a very active research topic at the moment which is a main focus of the proposed [EIC experiment](https://en.wikipedia.org/wiki/Electron%E2%80%93ion_collider)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f56f8e3-a866-49eb-aa1b-38c5b48d35e0",
   "metadata": {},
   "source": [
    "As quarks and leptons carry both electric charges and electro-weak charges they can also scatter using electro-weak bosons in addition to the photon.\n",
    "This leads to a fundamental distinction between two different types of DIS: we can either consider so-called Neutral Current (NC) DIS, where a boson without electric charge is exchanged (in the SM this corresponds to photons and Z-bosons), or Charged Current (CC) DIS, where an electrically chaged boson is exchanged (in the SM this corresponds to both W-bosons). As the coupling structure of those different types is completely different they probe different type of physics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e0043e-5617-48a2-bfdb-308ca33c1d6a",
   "metadata": {},
   "source": [
    "All in all, we can connect almost all particles (the Higgs is usually neglected in DIS experiments, as they do not operate at sufficient high energies) from the Standard model to the DIS process:\n",
    "| DIS | Standard Model | \n",
    "| :-: | :-: |\n",
    "| <img src=\"figures/pdg-dis.png\" width=560 height=260 /> | <img src=\"figures/Standard_Model_of_Elementary_Particles.svg.png\" width=260 height=260 /> |\n",
    "| taken from [PDG](https://pdg.lbl.gov/2023/web/viewer.html?file=../reviews/rpp2022-rev-structure-functions.pdf) | taken from [Wikipedia](https://en.wikipedia.org/wiki/Standard_Model) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147f4710-1677-4781-b13a-4ce636823974",
   "metadata": {},
   "source": [
    "As the exchanged boson is a virtual particle it does not only have a transverse polarization component, but also a longitudinal polarization component and we eventually have also to consider the spin structure of the particles. In practice this gives rise to three structure functions in total: $F_2$ (as discussed above), $F_L$ and $F_3$ - and all three can contribute to a total cross section (as measured in an experiment). In practice, experimentalists often normalize their measurements in such a way that the dominating contribution directly corresponds to $F_2$ (as is done in our example)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04abe0c2-3ca6-480e-9856-0cc15133d0d4",
   "metadata": {},
   "source": [
    "## Flavor number schemes (FNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6a9004-b065-4520-b459-0f3a1545c570",
   "metadata": {},
   "source": [
    "In our calculation above we assumed there are 5 light quark flavors taking part in the scattering - this assumption is only valid in a specific kinematic regime (this is exactly the region where we match the experimental data well). To be more precise: our calculation is valid if there is only a single relevant scale around (and that one scale is the virtuality $Q^2$) and there are five flavors actively running inside the proton.\n",
    "The other scales one might want to (and has to) consider are the quark masses $M$: if $Q^2 \\sim M^2$ we can not neglect these effects and we have to choose a different setup of the calculation. These considerations are commonly refered to as \"Flavor Number Schemes\" (FNS) and they are a common feature of higher order calculations (though in practice only relevant for DIS)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe47dfc-ab04-417f-963f-3bb8def81086",
   "metadata": {},
   "source": [
    "We can illustrate this with the following diagrams:\n",
    "\n",
    "<img src=\"figures/lo-light.png\" width=260 height=260 />  <img src=\"figures/lo-heavy.png\" width=260 height=260 /> \n",
    "\n",
    "In the first diagram (which we will call \"light\") the quark line refers to a massless quark of any flavor and this was the leading order diagram we used in our calculation above. The number of quark flavor which can enter this way into the calculation depends on the factorization scale: usually you consider a quark active if the factorization scale is above the quark mass.\n",
    "\n",
    "Instead, in the second diagram (which we will call \"heavy\") the quark line refers to a massive quark with mass $M$ of a specific flavor.\n",
    "Usually, one considers one quark at a time to be massive (while the others are either massless or infinitely massive). This diagram generates terms which behave as $Q^2/M^2$, so exactly the ones which are relevant in the region $Q^2 \\sim M^2$. However, we also know that in the limit of $Q^2 \\to \\infty$ we must have a connection to the first diagram, as we then can basically rewrite this diagram as the first diagram time a splitting $g\\to q\\bar q$.\n",
    "\n",
    "There is no unique way to resolve this ambiguity and different PDF groups have adopted different strategies: [FONLL](https://inspirehep.net/literature/842785) for NNPDF, [S-ACOT](https://inspirehep.net/literature/524600) for CTEQ and [TR](https://inspirehep.net/literature/709470) for MSHT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103b866d-dd76-4fda-b2cc-d9b5ae5f30d4",
   "metadata": {},
   "source": [
    "## Scale variations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bf4b87-9ba3-4461-9c2b-1e77f703c94c",
   "metadata": {},
   "source": [
    "In our calculation above we assumed there is only one scale in the game: the photon virtuality $Q^2$. Thus, we chose the renormalization scale $\\mu_R$ used for the strong couping $\\alpha_s(\\mu_R^2)$ to coincide with $Q^2$ and, likewise, we chose the factorization scale $\\mu_F$ used for the PDFs $f_j(z,\\mu_F^2)$ to coincide with $Q^2$. Either scale is an *unphysical* scale that was introduced in the mathematical procedure of factorization and (UV) renormalization with the statement that they should be related to a \"typical scale of the process\". We can choose this freedom in order to vary them and to obtain a theoretical uncertainty which can be used to estimated [missing higher order uncertainties](https://inspirehep.net/literature/1741422).\n",
    "\n",
    "Thus, we can write a more complete factorization formula:\n",
    "$$\n",
    "  \\sigma(x,Q^2,\\mu_F^2,\\mu_R^2) = \\sum_a\\sum_{n=0} \\left(\\frac{\\alpha_s(\\mu_R^2)}{4\\pi}\\right)^n \\int\\limits_x^1 \\frac{dz}{z} C_a^{(n)}(x/z, Q^2/\\mu_F^2, Q^2/\\mu_R^2) f_a^h(z,\\mu_F^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88018278-3d2f-4819-b438-b5c4cc083141",
   "metadata": {},
   "source": [
    "We can distinguish two different types of scale variations:\n",
    "- renormalization scale variations: these are related to the (perturbative) invariance of the (physical) structure function upon the variation of the (unphysical) renomalization scale. This procedure can be directly related to the renormalization group equation (RGE) of the strong coupling: the beta function $\\beta(\\alpha_s)$ (more on this in the next QCD tutorial!)\n",
    "- factorization scale variations: these are related to the factorization procedure and they can be performed in several ways. Again this can be directly related to the RGE of the PDF: the DGLAP equation (more on this in the next QCD tutorial!)\n",
    "\n",
    "In either case we can write:\n",
    "\n",
    "$$ \\frac {d\\sigma}{d\\mu^2} = 0 \\to \\frac {d\\sigma}{d\\mu^2} = O(\\alpha_s^{N+1}(\\mu^2)) $$,\n",
    "\n",
    "With $N$ the order of our calculation. So we replace the true independence with a higher order approximation. In practice, this amounts to varying the scale around $Q^2$ (e.g. $Q^2/2$ and $2Q^2$) and taking the results from these variations as uncertainties from missing higher orders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0a99be-6585-4d53-9ddb-bfc7ea5b2ff5",
   "metadata": {},
   "source": [
    "## Comparing our implementation to yadism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dee5f42-df28-4a0b-9670-aa107707ac2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from eko import interpolation\n",
    "import copy\n",
    "import yadism\n",
    "\n",
    "# While yadism can indeed do much more, we can use it also to check our simple implementation from above!\n",
    "\n",
    "# We need a \"theory card\" which describes all the theory settings,\n",
    "# such as masses of quarks and bosons, or coupling reference values.\n",
    "theory_card_simple = {\n",
    "    \"PTO\": 1,\n",
    "    \"FNS\": \"FFNS\",\n",
    "    \"DAMP\": 0,\n",
    "    \"IC\": 0,\n",
    "    \"IB\": 0,\n",
    "    \"ModEv\": \"TRN\",\n",
    "    \"ModSV\": \"unvaried\",\n",
    "    \"XIR\": 1.0,\n",
    "    \"XIF\": 1.0,\n",
    "    \"NfFF\": 5,\n",
    "    \"MaxNfAs\": 5,\n",
    "    \"MaxNfPdf\": 5,\n",
    "    \"Q0\": 1.65,\n",
    "    \"alphas\": 0.118,\n",
    "    \"Qref\": 91.2,\n",
    "    \"nf0\": 4,\n",
    "    \"nfref\": 5,\n",
    "    \"QED\": 0,\n",
    "    \"alphaqed\": 0.007496252,\n",
    "    \"Qedref\": 1.777,\n",
    "    \"SxRes\": 0,\n",
    "    \"SxOrd\": \"LL\",\n",
    "    \"HQ\": \"POLE\",\n",
    "    \"mc\": 1.51,\n",
    "    \"Qmc\": 1.51,\n",
    "    \"kcThr\": 1.0,\n",
    "    \"mb\": 4.92,\n",
    "    \"Qmb\": 4.92,\n",
    "    \"kbThr\": 1.0,\n",
    "    \"mt\": 172.5,\n",
    "    \"Qmt\": 172.5,\n",
    "    \"ktThr\": 1.0,\n",
    "    \"CKM\": \"0.97428 0.22530 0.003470 0.22520 0.97345 0.041000 0.00862 0.04030 0.999152\",\n",
    "    \"MZ\": 91.1876,\n",
    "    \"MW\": 80.398,\n",
    "    \"GF\": 1.1663787e-05,\n",
    "    \"SIN2TW\": 0.23126,\n",
    "    \"TMC\": 0,\n",
    "    \"MP\": 0.938,\n",
    "    \"global_nx\": 0,\n",
    "    \"EScaleVar\": 1,\n",
    "    \"kDIScThr\": 1.0,\n",
    "    \"kDISbThr\": 1.0,\n",
    "    \"kDIStThr\": 1.0,\n",
    "    \"n3lo_cf_variation\":1.0,\n",
    "}\n",
    "# We also need an \"observable card\" which tells us *what* we want to compute\n",
    "observables_card_simple = {\n",
    "    \"PolarizationDIS\": 0.0,\n",
    "    \"ProjectileDIS\": \"electron\",\n",
    "    \"PropagatorCorrection\": 0.0,\n",
    "    \"TargetDIS\": \"proton\",\n",
    "    \"interpolation_is_log\": True,\n",
    "    \"interpolation_polynomial_degree\": 4,\n",
    "    \"interpolation_xgrid\": interpolation.lambertgrid(60).tolist(),\n",
    "    \"observables\": {\"F2_light\": []},\n",
    "    \"prDIS\": \"EM\",\n",
    "    \"NCPositivityCharge\": None,\n",
    "}\n",
    "yadism.log.silent_mode = True\n",
    "\n",
    "\n",
    "def compare(curobs: list) -> pd.DataFrame:\n",
    "    \"\"\"Compare local implementation against yadism for given observables.\"\"\"\n",
    "    obs_card = copy.deepcopy(observables_card_simple)\n",
    "    # The limit, in which we were computing above, is called F2_light inside yadism\n",
    "    obs_card[\"observables\"][\"F2_light\"] = curobs\n",
    "    out = yadism.run_yadism(theory_card_simple, obs_card)\n",
    "    # as yadism produces a grid, we still need to apply the PDF\n",
    "    yad_data = out.apply_pdf_alphas_alphaqed_xir_xif(\n",
    "        pdf, pdf.alphasQ, lambda _: 0.0, 1.0, 1.0\n",
    "    )\n",
    "    cmp = []\n",
    "    for dat in yad_data[\"F2_light\"]:\n",
    "        # let's compute our own numbers\n",
    "        me = f2(dat[\"x\"], dat[\"Q2\"])\n",
    "        # and compare\n",
    "        cmp.append(\n",
    "            dict(\n",
    "                x=dat[\"x\"],\n",
    "                Q2=dat[\"Q2\"],\n",
    "                yadism=dat[\"result\"],\n",
    "                F2=me,\n",
    "                ratio=me / dat[\"result\"],\n",
    "            )\n",
    "        )\n",
    "    return pd.DataFrame.from_records(cmp)\n",
    "\n",
    "\n",
    "# let's take some random points in x and Q2 for our comparison\n",
    "yadism_vs_f2 = compare(\n",
    "    [\n",
    "        {\"x\": 0.001, \"Q2\": 10},\n",
    "        {\"x\": 0.001, \"Q2\": 100},\n",
    "        {\"x\": 0.001, \"Q2\": 1000},\n",
    "        {\"x\": 0.01, \"Q2\": 100},\n",
    "        {\"x\": 0.1, \"Q2\": 100},\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf4357e-2ad4-4048-8369-8dee164939b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "yadism_vs_f2\n",
    "# The remaining difference is most likely a left-over from the interpolation inside yadism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba4e038-a5da-4981-8cc4-8989d80bf16e",
   "metadata": {},
   "source": [
    "## Compare yadism to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df747ea-ab15-40ce-b88c-0cc80ebdf21f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from yadbox.export import dump_pineappl_to_file\n",
    "\n",
    "# Let's activate the full complications and recompute!\n",
    "\n",
    "theory_card_full = {\n",
    "    \"PTO\": 2,\n",
    "    \"FNS\": \"FONLL-C\",\n",
    "    \"DAMP\": 0,\n",
    "    \"IC\": 1,\n",
    "    \"IB\": 0,\n",
    "    \"ModEv\": \"TRN\",\n",
    "    \"ModSV\": \"unvaried\",\n",
    "    \"XIR\": 1.0,\n",
    "    \"XIF\": 1.0,\n",
    "    \"NfFF\": 5,\n",
    "    \"MaxNfAs\": 5,\n",
    "    \"MaxNfPdf\": 5,\n",
    "    \"Q0\": 1.65,\n",
    "    \"alphas\": 0.118,\n",
    "    \"Qref\": 91.2,\n",
    "    \"nf0\": 4,\n",
    "    \"nfref\": 5,\n",
    "    \"QED\": 0,\n",
    "    \"alphaqed\": 0.007496252,\n",
    "    \"Qedref\": 1.777,\n",
    "    \"SxRes\": 0,\n",
    "    \"SxOrd\": \"LL\",\n",
    "    \"HQ\": \"POLE\",\n",
    "    \"mc\": 1.51,\n",
    "    \"Qmc\": 1.51,\n",
    "    \"kcThr\": 1.0,\n",
    "    \"mb\": 4.92,\n",
    "    \"Qmb\": 4.92,\n",
    "    \"kbThr\": 4.0,\n",
    "    \"mt\": 172.5,\n",
    "    \"Qmt\": 172.5,\n",
    "    \"ktThr\": 1.0,\n",
    "    \"CKM\": \"0.97428 0.22530 0.003470 0.22520 0.97345 0.041000 0.00862 0.04030 0.999152\",\n",
    "    \"MZ\": 91.1876,\n",
    "    \"MW\": 80.398,\n",
    "    \"GF\": 1.1663787e-05,\n",
    "    \"SIN2TW\": 0.23126,\n",
    "    \"TMC\": 1,\n",
    "    \"MP\": 0.938,\n",
    "    \"global_nx\": 0,\n",
    "    \"EScaleVar\": 1,\n",
    "    \"kDIScThr\": 1.0,\n",
    "    \"kDISbThr\": 4.0,\n",
    "    \"kDIStThr\": 1.0,\n",
    "}\n",
    "observables_card_full = {\n",
    "    \"PolarizationDIS\": 0.0,\n",
    "    \"ProjectileDIS\": \"electron\",\n",
    "    \"PropagatorCorrection\": 0.0,\n",
    "    \"TargetDIS\": \"proton\",\n",
    "    \"interpolation_is_log\": True,\n",
    "    \"interpolation_polynomial_degree\": 4,\n",
    "    \"interpolation_xgrid\": interpolation.lambertgrid(60).tolist(),\n",
    "    \"observables\": {\"XSHERANC_total\": []},\n",
    "    \"prDIS\": \"NC\",\n",
    "    \"NCPositivityCharge\": None,\n",
    "}\n",
    "\n",
    "\n",
    "def compute():\n",
    "    \"\"\"Compute yadism prediction for all experimental bins.\"\"\"\n",
    "    obs_card = copy.deepcopy(observables_card_full)\n",
    "    # collect all kinematics\n",
    "    curobs = data.apply(\n",
    "        lambda dat: {\"x\": dat[\"x\"], \"Q2\": dat[\"Q2\"], \"y\": dat[\"y\"]},\n",
    "        axis=1,\n",
    "    )\n",
    "    obs_card[\"observables\"][\"XSHERANC_total\"] = list(curobs.to_dict().values())\n",
    "    out = yadism.run_yadism(theory_card_full, obs_card)\n",
    "    # let's dump the grid, which we can reuse\n",
    "    dump_pineappl_to_file(\n",
    "        out, \"HERA_NC_318GEV_EM_SIGMARED.pineappl.lz4\", \"XSHERANC_total\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Attention! Since we're asking for a more complicated thing the calculation will actually take a bit (~20min) ...\n",
    "# compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a20fa01-f340-41dc-aff9-bb58178aca00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the precomputed grid instead!\n",
    "# we load it ...\n",
    "grid = pineappl.grid.Grid.read(\"HERA_NC_318GEV_EM_SIGMARED.pineappl.lz4\")\n",
    "# and convolute it with the PDF - finish!\n",
    "yadism_data = grid.convolve_with_one(2212, pdf.xfxQ2, pdf.alphasQ2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e268b73-4946-48f3-8a31-6aa2b9b2b248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot the experimental data against our predictions!\n",
    "plt.close()\n",
    "# let's join the experimental numbers, our numbers computed above, and yadism\n",
    "plot_data = pd.concat(\n",
    "    [\n",
    "        data[\"Q2\"],\n",
    "        data[\"x\"],\n",
    "        data[\"Sigma\"],\n",
    "        f2_data[\"F2\"],\n",
    "        pd.DataFrame([yadism_data], index=[\"yadism\"]).T,\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "fig, axes = plt.subplots(6, 4, figsize=(10, 15), clear=True, layout=\"tight\")\n",
    "fig.suptitle(r\"HERA $e^- + p \\to e^- + X$ with $\\sqrt{s}=318 \\mathrm{GeV}$\")\n",
    "# again, iterate by Q2\n",
    "for q2, ax in zip(plot_data[\"Q2\"].unique(), axes.flatten()):\n",
    "    dat = plot_data[plot_data[\"Q2\"] == q2]\n",
    "    ax.set_title(f\"$Q^2 = {q2} \\\\mathrm{{GeV}}^2$\")\n",
    "    # now, we can compare the three numbers: from experiment, from above, and from yadism\n",
    "    (pexp,) = ax.plot(dat[\"x\"], dat[\"Sigma\"], \"o\", label=\"exp\")\n",
    "    (pth,) = ax.plot(dat[\"x\"], dat[\"F2\"], \"x\", label=\"th\")\n",
    "    (pyad,) = ax.plot(dat[\"x\"], dat[\"yadism\"], \"rs\", label=\"yadism\", markersize=3)\n",
    "    ax.set_xscale(\"log\")\n",
    "fig.legend(handles=[pexp, pth, pyad], ncols=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a265425d-7dac-4b8e-a3ff-42ea9a06f039",
   "metadata": {},
   "source": [
    "We can see some improvement in the small $Q^2$ region, where our naive implementation was missing charm mass effects, and some improvements in the large\n",
    "$Q^2$ region, where, instead, we were missing electro-weak corrections (from the Z-boson).\n",
    "\n",
    "Note that we are not dealing with uncertainties here, neither on the experimental side nor on the theory side."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
